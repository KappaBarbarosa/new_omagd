"""
Graph Discrete Diffusion Model for HARL VQ Architecture
åŸºæ–¼åŸæœ¬ mask_discrete_diffusion.py æ¶æ§‹ï¼Œé©é…åˆ° VQ Graph tokens

ä¸»è¦åŠŸèƒ½ï¼š
1. è™•ç† Tokenizer ç”¢ç”Ÿçš„ discrete tokens [B, N]
2. å¯¦ç¾å®Œæ•´çš„ discrete diffusion æµç¨‹ (forward/reverse process)
3. æ”¯æŒè¤‡é›œçš„ç¯€é»æ„ŸçŸ¥ masking ç­–ç•¥
4. èˆ‡ HARL VQ è¨“ç·´æ¡†æ¶å…¼å®¹
5. ä½¿ç”¨ SUBS åƒæ•¸åŒ–æ–¹å¼
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchmetrics
from typing import Dict, Optional
from dataclasses import dataclass
from loguru import logger

from utils.noise_schedule import Linear as LinearNoise
from modules.graph_reconstructers.graph_diffusion_transformer import GraphDiffusionTransformer


LOG2 = math.log(2)


@dataclass
class DiffusionLoss:
    """Loss dataclass for diffusion training"""

    loss: torch.FloatTensor
    nlls: torch.FloatTensor
    token_mask: torch.FloatTensor


class NLL(torchmetrics.aggregation.MeanMetric):
    """Negative log-likelihood metric"""

    pass


class BPD(NLL):
    """Bits per dimension metric"""

    def compute(self) -> torch.Tensor:
        return self.mean_value / self.weight / LOG2


class Perplexity(NLL):
    """Perplexity metric"""

    def compute(self) -> torch.Tensor:
        return torch.exp(self.mean_value / self.weight)


class GraphDiscreteDiffusion(nn.Module):
    """
    Graph Discrete Diffusion Model based on mask_discrete_diffusion.py
    """

    def __init__(
        self,
        vocab_size: int,
        config: Optional[Dict] = None,
        embed_dim: int = 256,
        num_heads: int = 8,
        num_layers: int = 6,
        dropout: float = 0.1,
        max_seq_len: int = 50,
        device: torch.device = torch.device("cpu"),
        input_mode: str = "token",  # "token" or "feature"
        feature_dim: int = None,  # Required if input_mode="feature"
    ):
        super().__init__()

        self.vocab_size = vocab_size
        self.device = device
        self.input_mode = input_mode

        # Mask token handling (æ·»åŠ ä¸€å€‹æ–°çš„ mask token)
        self.mask_index = vocab_size  # ä½¿ç”¨ vocab_size ä½œç‚º mask token ID
        self.actual_vocab_size = vocab_size + 1

        # Configuration
        self.config = config or {}
        self.importance_sampling = self.config.get("importance_sampling", False)
        self.change_of_variables = self.config.get("change_of_variables", False)
        self.time_conditioning = self.config.get("time_conditioning", True)

        # Noise schedule (ä½¿ç”¨ç·šæ€§ schedule)
        sigma_min = self.config.get("sigma_min", 1e-3)
        sigma_max = self.config.get("sigma_max", 1.0)
        self.noise = LinearNoise(sigma_min=sigma_min, sigma_max=sigma_max)

        # Transformer backbone
        self.backbone = GraphDiffusionTransformer(
            vocab_size=self.actual_vocab_size,
            embed_dim=embed_dim,
            num_heads=num_heads,
            num_layers=num_layers,
            dropout=dropout,
            max_seq_len=max_seq_len,
            input_mode=input_mode,
            feature_dim=feature_dim,
        )

        # Metrics
        metrics = torchmetrics.MetricCollection(
            {
                "nll": NLL(),
                "bpd": BPD(),
                "ppl": Perplexity(),
            }
        )
        metrics.set_dtype(torch.float64)
        self.train_metrics = metrics.clone(prefix="train/")
        self.valid_metrics = metrics.clone(prefix="val/")

        self.neg_infinity = -1000000.0

        logger.info("ğŸŒŠ [GRAPH_DISCRETE_DIFFUSION] Initialized with:")
        logger.info(f"  - vocab_size: {vocab_size}, mask_token_id: {self.mask_index}")
        logger.info(f"  - actual_vocab_size: {self.actual_vocab_size}")
        logger.info(f"  - parameterization: SUBS")
        logger.info(f"  - input_mode: {input_mode}")
        logger.info(
            f"  - embed_dim: {embed_dim}, num_heads: {num_heads}, num_layers: {num_layers}"
        )
        if input_mode == "feature":
            logger.info(f"  - feature_dim: {feature_dim}")

        self.to(device)

    def q_xt(self, x, move_chance):
        """
        å‰å‘å™ªè²éç¨‹ï¼šå°‡åŸå§‹tokenæŒ‰æ¦‚ç‡æ›¿æ›ç‚ºmask token

        Args:
            x: [batch_size, seq_len] - original tokens
            move_chance: [batch_size, 1] or [batch_size] - probability of masking

        Returns:
            xt: [batch_size, seq_len] - noisy tokens
        """
        if move_chance.dim() == 1:
            move_chance = move_chance.unsqueeze(-1)

        move_indices = torch.rand(*x.shape, device=x.device) < move_chance
        xt = torch.where(move_indices, self.mask_index, x)
        return xt

    def _process_sigma(self, sigma):
        """Process sigma for time conditioning"""
        if sigma is None:
            return sigma
        if sigma.ndim > 1:
            sigma = sigma.squeeze(-1)
        if not self.time_conditioning:
            sigma = torch.zeros_like(sigma)
        assert sigma.ndim == 1, sigma.shape
        return sigma

    def forward(self, x, sigma):
        """
        Forward pass through the diffusion model

        Args:
            x: [batch_size, seq_len] - input tokens
            sigma: [batch_size] - noise level

        Returns:
            log_score: [batch_size, seq_len, vocab_size] - log probabilities
        """
        sigma = self._process_sigma(sigma)
        logits = self.backbone(x, sigma)

        return self._subs_parameterization(logits=logits, xt=x)

    def _subs_parameterization(self, logits, xt):
        """SUBS parameterization from original implementation"""
        # log prob at the mask index = - infinity
        logits[:, :, self.mask_index] += self.neg_infinity

        # Normalize the logits such that x.exp() is
        # a probability distribution over vocab_size.
        logits = logits - torch.logsumexp(logits, dim=-1, keepdim=True)

        # Apply updates directly in the logits matrix.
        # For the logits of the unmasked tokens, set all values
        # to -infinity except for the indices corresponding to
        # the unmasked tokens.
        unmasked_indices = xt != self.mask_index
        logits[unmasked_indices] = self.neg_infinity
        logits[unmasked_indices, xt[unmasked_indices]] = 0
        return logits

    def _sample_t(self, n, device):
        """ç°¡å–®çš„æ™‚é–“æ­¥æ¡æ¨£"""
        return torch.rand(n, device=device)

    def _forward_pass_diffusion(self, x0):
        """
        æ¨™æº– Discrete Diffusion Forward Pass


        Args:
            x0: [batch_size, num_nodes] - clean tokens

        Returns:
            dict with loss and diffusion info
        """
        if self.input_mode == "token":
            batch_size, num_nodes = x0.shape
        elif self.input_mode == "feature":
            batch_size, num_nodes, _ = x0.shape
        else:
            raise ValueError(f"Unknown input_mode: {self.input_mode}")

        device = x0.device

        t = self._sample_t(batch_size, device)

        sigma, dsigma = self.noise(t)  # sigma: [B], dsigma: [B] or scalar

        if not torch.is_tensor(sigma):
            sigma = torch.tensor(sigma, device=device)
        if sigma.ndim == 0:  # æ ‡é‡å¼ é‡ â†’ æ‰©å±•ä¸º [B]
            sigma = sigma.expand(batch_size)
        elif sigma.ndim > 1:  # å¤šç»´å¼ é‡ â†’ å‹ç¼©ä¸º [B]
            sigma = sigma.squeeze()

        if not torch.is_tensor(dsigma):
            dsigma = torch.tensor(dsigma, device=device)
        if dsigma.ndim == 0:  # æ ‡é‡å¼ é‡ â†’ æ‰©å±•ä¸º [B]
            dsigma = dsigma.expand(batch_size)
        elif dsigma.ndim > 1:  # å¤šç»´å¼ é‡ â†’ å‹ç¼©ä¸º [B]
            dsigma = dsigma.squeeze()

        move_chance = 1 - torch.exp(-sigma[:, None])  # [B, 1] - ä¸ mdlm ä¿æŒä¸€è‡´
        xt = self.q_xt(x0, move_chance)

        model_output = self.forward(xt, sigma)

        # SUBS è¿ç»­æ—¶é—´ loss
        log_p_theta = torch.gather(
            input=model_output, dim=-1, index=x0[:, :, None]
        ).squeeze(-1)

        if self.change_of_variables or self.importance_sampling:
            loss = log_p_theta * torch.log1p(-torch.exp(-self.noise.sigma_min))
        else:
            loss = -log_p_theta * (dsigma / torch.expm1(sigma))[:, None]

        # ç»Ÿä¸€è¿”å› loss [B, N]
        return loss

    def compute_loss(
        self,
        tokens: torch.Tensor,
        useless_mask: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> DiffusionLoss:
        """
        Discrete Diffusion Loss é©é…éƒ¨åˆ†è§€æ¸¬å ´æ™¯

        Args:
            tokens: [batch_size, num_nodes] - clean tokens
            useless_mask: [batch_size] - useless sample mask (True=ç„¡æ„ç¾©æ¨£æœ¬)

        Returns:
            DiffusionLoss: dataclass containing scalar loss, per-token nlls, and token mask
        """

        # è¨ˆç®— per-token diffusion loss [B, N]
        loss_per_token = self._forward_pass_diffusion(tokens)

        # å‰µå»º token-level mask [B, N]
        # 1 è¡¨ç¤ºè©² token çš„ loss æ‡‰è©²è¢«è¨ˆç®—ï¼Œ0 è¡¨ç¤ºå¿½ç•¥
        token_mask = torch.ones_like(loss_per_token, dtype=torch.float32)

        if useless_mask is not None:
            # å°‡ç„¡æ„ç¾©æ¨£æœ¬çš„æ‰€æœ‰ token mask è¨­ç‚º 0
            token_mask[useless_mask] = 0.0

        # æ‡‰ç”¨ maskï¼šåªä¿ç•™æœ‰æ•ˆ token çš„ loss
        masked_nlls = loss_per_token * token_mask  # [B, N]

        # è¨ˆç®—å¹³å‡ lossï¼ˆåªå°æœ‰æ•ˆ token æ±‚å¹³å‡ï¼‰
        num_valid_tokens = token_mask.sum()
        if num_valid_tokens > 0:
            scalar_loss = masked_nlls.sum() / num_valid_tokens
        else:
            # é˜²æ­¢é™¤é›¶ï¼šå¦‚æœæ²’æœ‰æœ‰æ•ˆ tokenï¼Œè¿”å› 0 loss
            scalar_loss = torch.tensor(0.0, device=tokens.device, requires_grad=True)
            logger.warning("âš ï¸  No valid tokens for loss computation!")
        loss = DiffusionLoss(
            loss=scalar_loss,  # scalar for backprop
            nlls=masked_nlls,  # [B, N] for metrics
            token_mask=token_mask,  # [B, N] for tracking
        )
        return {"loss": loss.loss, "logs": {}}

    def reconstruct_hidden_tokens(
        self,
        visible_tokens: torch.Tensor,
        visible_mask: torch.Tensor,
        num_steps: int = 20,
        temperature: float = 1.0,
    ) -> torch.Tensor:
        """
        Discrete Diffusion æ¡æ¨£æ¨ç†éç¨‹

        Args:
            visible_tokens: [B, N] - å¯è¦‹çš„ tokens
            visible_mask: [B, N] - å¯è¦‹æ€§ mask (1=å¯è¦‹, 0=éœ€è¦é‡æ§‹)
            num_steps: diffusion æ¡æ¨£æ­¥æ•¸
            temperature: æ¡æ¨£æº«åº¦

        Returns:
            reconstructed_tokens: [B, N] - é‡æ§‹çš„å®Œæ•´ tokens
        """
        batch_size, num_nodes = visible_tokens.shape
        device = visible_tokens.device

        # 1. åˆå§‹åŒ–ï¼šhidden ä½ç½®è¨­ç‚º mask token
        tokens = visible_tokens.clone()
        tokens[visible_mask == 0] = self.mask_index

        # 2. å‰µå»ºæ™‚é–“æ­¥åºåˆ—ï¼ˆå¾é«˜å™ªè²åˆ°ä½å™ªè²ï¼‰
        timesteps = torch.linspace(1.0, 0.0, num_steps + 1, device=device)[
            :-1
        ]  # ä¸åŒ…æ‹¬ t=0

        # 3. é€æ­¥å»å™ª
        for i, t in enumerate(timesteps):
            t_batch = t.expand(batch_size)  # [B]

            with torch.no_grad():
                # æ¨¡å‹é æ¸¬
                logits = self.forward(tokens, t_batch)  # [B, N, vocab_size]

                # åªå° hidden ä½ç½®é€²è¡Œæ›´æ–°
                hidden_positions = visible_mask == 0

                if hidden_positions.any():
                    # è¨ˆç®—ç•¶å‰æ­¥çš„ masking æ¦‚ç‡
                    # éš¨è‘—æ™‚é–“æ¸›å°‘ï¼Œmasking æ¦‚ç‡é™ä½
                    current_mask_prob = t * 0.8  # æœ€å¤§ 0.8

                    # å°æ–¼æ¯å€‹ hidden ä½ç½®ï¼Œæ±ºå®šæ˜¯å¦ä¿æŒ mask æˆ–æ¡æ¨£æ–° token
                    should_unmask = (
                        torch.rand_like(tokens, dtype=torch.float) > current_mask_prob
                    )
                    update_positions = hidden_positions & should_unmask

                    if update_positions.any():
                        # æº«åº¦æ¡æ¨£
                        probs = F.softmax(logits / temperature, dim=-1)

                        # åªå°éœ€è¦æ›´æ–°çš„ä½ç½®é€²è¡Œæ¡æ¨£
                        update_flat = update_positions.view(-1)
                        probs_flat = probs.view(-1, self.actual_vocab_size)

                        sampled_tokens = torch.multinomial(
                            probs_flat[update_flat], num_samples=1
                        ).squeeze(-1)

                        # æ›´æ–° tokens
                        tokens_flat = tokens.view(-1)
                        tokens_flat[update_flat] = sampled_tokens
                        tokens = tokens_flat.view(batch_size, num_nodes)

                # ç¢ºä¿ visible ä½ç½®ä¿æŒä¸è®Š
                tokens[visible_mask == 1] = visible_tokens[visible_mask == 1]

        # 4. æœ€çµ‚æ¸…ç†ï¼šå¦‚æœé‚„æœ‰ mask tokenï¼Œç”¨æœ€é«˜æ¦‚ç‡é æ¸¬
        final_hidden = tokens == self.mask_index
        if final_hidden.any():
            with torch.no_grad():
                final_logits = self.forward(
                    tokens, torch.zeros(batch_size, device=device)
                )
                final_pred = torch.argmax(final_logits, dim=-1)
                tokens[final_hidden] = final_pred[final_hidden]

        return tokens

